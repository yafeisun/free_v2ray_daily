#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
èŠ‚ç‚¹æµ‹é€Ÿè„šæœ¬ - ä½¿ç”¨çº¿ç¨‹æ± åˆ†æ‰¹æµ‹è¯•
"""

import sys
import os
import subprocess
import time
import yaml
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.utils.logger import get_logger


class BatchNodeTester:
    """åŸºäºçº¿ç¨‹æ± çš„åˆ†æ‰¹èŠ‚ç‚¹æµ‹è¯•å™¨"""
    
    def __init__(self, project_root: str = None):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.logger = get_logger("batch_tester")
        
        # è®¾ç½®é¡¹ç›®æ ¹ç›®å½•
        if project_root is None:
            self.project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        else:
            self.project_root = project_root
        
        # è·¯å¾„é…ç½®
        self.subscheck_dir = os.path.join(self.project_root, 'subscheck')
        self.binary_path = os.path.join(self.subscheck_dir, 'bin', 'subs-check')
        self.config_dir = os.path.join(self.subscheck_dir, 'config')
        self.output_dir = os.path.join(self.project_root, 'output')
        
        # æµ‹è¯•é…ç½®
        self.batch_size = 100  # æ¯æ‰¹èŠ‚ç‚¹æ•°
        self.max_workers = 2  # å¹¶å‘æ‰¹æ¬¡æ•°
        self.concurrent = 5  # æ¯ä¸ªæ‰¹æ¬¡çš„å¹¶å‘æ•°ï¼ˆé™ä½ä»¥å‡å°‘å¤±è´¥ç‡ï¼‰
        
        # æµ‹è¯•è¶…æ—¶ï¼ˆæ¯æ‰¹ï¼‰
        self.batch_timeout = 1800  # 30åˆ†é’Ÿ
    
    def create_unified_config(self) -> str:
        """åˆ›å»ºç»Ÿä¸€çš„é…ç½®æ–‡ä»¶ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å…±äº«ï¼‰"""
        config_file = os.path.join(self.config_dir, 'batch_config.yaml')
        
        config = {
            # åŸºæœ¬é…ç½®
            'print-progress': True,
            'concurrent': self.concurrent,
            'check-interval': 999999,
            'timeout': 10000,  # å¢åŠ åˆ°10ç§’
            
            # æµ‹é€Ÿé…ç½®
            'alive-test-url': 'http://gstatic.com/generate_204',
            'speed-test-url': '',
            'min-speed': 0,
            'download-timeout': 1,
            'download-mb': 0,
            'total-speed-limit': 0,
            
            # æµåª’ä½“æ£€æµ‹
            'media-check': True,
            'media-check-timeout': 8,
            'platforms': ['youtube', 'openai', 'gemini'],
            
            # èŠ‚ç‚¹é…ç½®
            'rename-node': True,
            'node-prefix': '',
            'success-limit': 0,
            
            # è¾“å‡ºé…ç½®
            'output-dir': self.output_dir,
            'listen-port': '',
            'save-method': 'local',
            
            # Web UI
            'enable-web-ui': False,
            'api-key': '',
            
            # Sub-Store
            'sub-store-port': '',
            'sub-store-path': '',
            
            # ä»£ç†é…ç½®
            'github-proxy': '',
            'proxy': '',
            
            # å…¶ä»–
            'keep-success-proxies': False,
            'sub-urls-retry': 3,
            'sub-urls-get-ua': 'clash.meta (https://github.com/beck-8/subs-check)',
            
            # è®¢é˜…é“¾æ¥ï¼ˆç»Ÿä¸€é…ç½®ï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–ï¼‰
            'sub-urls': []
        }
        
        # ä¿å­˜é…ç½®
        os.makedirs(os.path.dirname(config_file), exist_ok=True)
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config, f, allow_unicode=True, default_flow_style=False)
        
        self.logger.info(f"ç»Ÿä¸€é…ç½®æ–‡ä»¶å·²åˆ›å»º: {config_file}")
        return config_file
    
    def clean_old_configs(self):
        """æ¸…ç†æ—§çš„æ‰¹æ¬¡é…ç½®æ–‡ä»¶"""
        import glob
        old_configs = glob.glob(os.path.join(self.config_dir, 'batch_*.yaml'))
        for old_config in old_configs:
            try:
                os.remove(old_config)
                self.logger.info(f"æ¸…ç†æ—§é…ç½®æ–‡ä»¶: {old_config}")
            except Exception as e:
                self.logger.warning(f"æ¸…ç†é…ç½®æ–‡ä»¶å¤±è´¥ {old_config}: {str(e)}")
    
    def run_single_batch(self, batch_nodes: List[str], batch_index: int, http_server_port: int) -> List[str]:
        """è¿è¡Œå•ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•"""
        self.logger.info(f"å¼€å§‹æµ‹è¯•æ‰¹æ¬¡ {batch_index}ï¼ŒèŠ‚ç‚¹æ•°: {len(batch_nodes)}")
        
        try:
            # ä¸ºå½“å‰æ‰¹æ¬¡åˆ›å»ºç‹¬ç«‹çš„è®¢é˜…æ–‡ä»¶
            batch_subscription_file = os.path.join(self.project_root, 'result', f'batch_subscription_{batch_index}.yaml')
            from scripts import convert_nodes_to_subscription
            batch_clash_config = convert_nodes_to_subscription.convert_nodes_to_clash(batch_nodes)
            with open(batch_subscription_file, 'w', encoding='utf-8') as f:
                yaml.dump(batch_clash_config, f, allow_unicode=True, default_flow_style=False)
            
            # ä½¿ç”¨ç»Ÿä¸€é…ç½®æ–‡ä»¶ï¼Œé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šè®¢é˜…URL
            config_file = os.path.join(self.config_dir, 'batch_config.yaml')
            subscription_url = f'http://127.0.0.1:{http_server_port}/result/batch_subscription_{batch_index}.yaml'
            
            # è¿è¡Œsubs-checkï¼Œä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–è®¢é˜…URL
            cmd = [self.binary_path, '-f', config_file, '--sub-url', subscription_url]
            
            self.logger.info(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                cwd=self.project_root,
                universal_newlines=False,
                bufsize=0
            )
            
            # å®æ—¶è¾“å‡ºæ—¥å¿—
            start_time = time.time()
            last_output_time = start_time
            last_line = ""
            last_progress = 0
            last_progress_time = start_time
            
            while True:
                # æ£€æŸ¥è¶…æ—¶
                elapsed = time.time() - start_time
                if elapsed > self.batch_timeout:
                    self.logger.error(f"æ‰¹æ¬¡ {batch_index} è¶…æ—¶ï¼ˆ{self.batch_timeout}ç§’ï¼‰ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    process.terminate()
                    process.wait(timeout=10)
                    break
                
                # æ£€æŸ¥é™é»˜è¶…æ—¶
                if time.time() - last_output_time > 300:
                    self.logger.info(f"æ‰¹æ¬¡ {batch_index} 300ç§’æ— è¾“å‡ºï¼Œè®¤ä¸ºå·²å®Œæˆ")
                    break
                
                # æ£€æŸ¥è¿›åº¦åœæ»ï¼ˆè¶…è¿‡90%ä¸”120ç§’æ— å˜åŒ–ï¼‰
                if last_progress >= 90.0 and (time.time() - last_progress_time) > 120:
                    self.logger.warning(f"æ‰¹æ¬¡ {batch_index} è¿›åº¦åœæ»åœ¨ {last_progress}% è¶…è¿‡120ç§’ï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                    process.terminate()
                    process.wait(timeout=10)
                    break
                
                # è¯»å–è¾“å‡º
                try:
                    import select
                    ready, _, _ = select.select([process.stdout], [], [], 1.0)
                    if ready:
                        byte = process.stdout.read(1)
                        if byte:
                            last_output_time = time.time()
                            char = byte.decode('utf-8', errors='ignore')
                            if char == '\n':
                                if last_line.strip():
                                    print(f"[æ‰¹æ¬¡{batch_index}] {last_line.strip()}")
                                    # è§£æè¿›åº¦
                                    import re
                                    progress_match = re.search(r'\[.*?\]\s+(\d+\.?\d*)%\s+\((\d+)/(\d+)\)', last_line)
                                    if progress_match:
                                        current_progress = float(progress_match.group(1))
                                        if current_progress > last_progress:
                                            last_progress = current_progress
                                            last_progress_time = time.time()
                                last_line = ""
                            elif char == '\r':
                                if last_line.strip():
                                    print(f"[æ‰¹æ¬¡{batch_index}] {last_line.strip()}")
                                    # è§£æè¿›åº¦ï¼ˆå›è½¦ç¬¦ä¹Ÿå¯èƒ½åŒ…å«è¿›åº¦ä¿¡æ¯ï¼‰
                                    import re
                                    progress_match = re.search(r'\[.*?\]\s+(\d+\.?\d*)%\s+\((\d+)/(\d+)\)', last_line)
                                    if progress_match:
                                        current_progress = float(progress_match.group(1))
                                        if current_progress > last_progress:
                                            last_progress = current_progress
                                            last_progress_time = time.time()
                                last_line = ""
                            else:
                                last_line += char
                        else:
                            break
                except (OSError, ValueError):
                    break
                
                # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ç»“æŸ
                if process.poll() is not None:
                    break
                
                time.sleep(0.01)
            
            # ç­‰å¾…è¿›ç¨‹ç»“æŸ
            return_code = process.wait(timeout=30)
            self.logger.info(f"æ‰¹æ¬¡ {batch_index} å®Œæˆï¼Œè¿”å›ç : {return_code}")
            
            # è§£æç»“æœ
            output_file = os.path.join(self.output_dir, 'all.yaml')
            if os.path.exists(output_file):
                results = self.parse_results(output_file)
                self.logger.info(f"æ‰¹æ¬¡ {batch_index} æœ‰æ•ˆèŠ‚ç‚¹æ•°: {len(results)}")
                return results
            else:
                self.logger.warning(f"æ‰¹æ¬¡ {batch_index} è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                return []
            
        except Exception as e:
            self.logger.error(f"æ‰¹æ¬¡ {batch_index} æµ‹è¯•å¤±è´¥: {str(e)}")
            return []
    
    def parse_results(self, output_file: str) -> List[str]:
        """è§£ææµ‹è¯•ç»“æœ"""
        try:
            with open(output_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            results = []
            if data and 'proxies' in data:
                for proxy in data['proxies']:
                    # æå–åª’ä½“ä¿¡æ¯
                    media_info = self._extract_media_info(proxy)
                    
                    # 2é€‰1è§„åˆ™ï¼šGPTæˆ–Geminiè‡³å°‘é€šè¿‡1ä¸ª
                    if media_info['gpt'] or media_info['gemini']:
                        # ç”Ÿæˆæ–°åç§°
                        region = self._extract_region(proxy)
                        region_number = self._extract_region_number(proxy)
                        new_name = self._generate_node_name(region, region_number, media_info)
                        
                        # è½¬æ¢ä¸ºV2Ray URI
                        v2ray_uri = self._convert_proxy_to_uri(proxy, new_name)
                        if v2ray_uri:
                            results.append(v2ray_uri)
            
            return results
        
        except Exception as e:
            self.logger.error(f"è§£æç»“æœå¤±è´¥: {str(e)}")
            return []
    
    def _extract_region(self, proxy: dict) -> str:
        """æå–åœ°åŒºä¿¡æ¯"""
        import re
        name = proxy.get('name', '')
        match = re.search(r'[ğŸ‡¦-ğŸ‡¿]{2}([A-Z]{2})_\d+', name)
        if match:
            return match.group(1)
        return 'US'
    
    def _extract_region_number(self, proxy: dict) -> int:
        """æå–åœ°åŒºç¼–å·"""
        import re
        name = proxy.get('name', '')
        match = re.search(r'[ğŸ‡¦-ğŸ‡¿]{2}[A-Z]{2}_(\d+)', name)
        if match:
            return int(match.group(1))
        return 1
    
    def _extract_media_info(self, proxy: dict) -> dict:
        """æå–åª’ä½“æµ‹è¯•ç»“æœ"""
        media_info = {'gpt': False, 'gemini': False, 'youtube': False}
        name = proxy.get('name', '')
        
        if 'GPTâº' in name:
            media_info['gpt'] = True
        if 'GM' in name:
            media_info['gemini'] = True
        if '|YT-' in name:
            media_info['youtube'] = True
        
        return media_info
    
    def _generate_node_name(self, region: str, number: int, media_info: dict) -> str:
        """ç”ŸæˆèŠ‚ç‚¹åç§°"""
        flags = {'HK': 'ğŸ‡­ğŸ‡°', 'US': 'ğŸ‡ºğŸ‡¸', 'JP': 'ğŸ‡¯ğŸ‡µ', 'SG': 'ğŸ‡¸ğŸ‡¬', 'TW': 'ğŸ‡¨ğŸ‡³', 'KR': 'ğŸ‡°ğŸ‡·'}
        flag = flags.get(region, '')
        
        ai_tag = ''
        if media_info['gpt'] and media_info['gemini']:
            ai_tag = 'GPT|GM'
        elif media_info['gpt']:
            ai_tag = 'GPT'
        elif media_info['gemini']:
            ai_tag = 'GM'
        
        yt_tag = '|YT' if media_info['youtube'] else ''
        
        return f"{flag}{region}_{number}|{ai_tag}{yt_tag}"
    
    def _convert_proxy_to_uri(self, proxy: dict, new_name: str) -> str:
        """è½¬æ¢ClashèŠ‚ç‚¹ä¸ºV2Ray URI"""
        try:
            proxy_type = proxy.get('type', '')
            
            if proxy_type == 'ss':
                cipher = proxy.get('cipher', 'aes-256-gcm')
                password = proxy.get('password', '')
                server = proxy.get('server', '')
                port = proxy.get('port', 443)
                return f"ss://{cipher}:{password}@{server}:{port}#{new_name}"
            
            elif proxy_type == 'vless':
                uuid = proxy.get('uuid', '')
                server = proxy.get('server', '')
                port = proxy.get('port', 443)
                params = ['encryption=none', 'security=tls', 'type=tcp']
                uri = f"vless://{uuid}@{server}:{port}?{'&'.join(params)}#{new_name}"
                return uri
            
            elif proxy_type == 'trojan':
                password = proxy.get('password', '')
                server = proxy.get('server', '')
                port = proxy.get('port', 443)
                params = ['security=tls']
                uri = f"trojan://{password}@{server}:{port}?{'&'.join(params)}#{new_name}"
                return uri
            
            elif proxy_type == 'hysteria2':
                password = proxy.get('password', '')
                server = proxy.get('server', '')
                port = proxy.get('port', 443)
                uri = f"hysteria2://{password}@{server}:{port}?insecure=1#{new_name}"
                return uri
            
            return ''
        
        except Exception as e:
            self.logger.error(f"è½¬æ¢èŠ‚ç‚¹å¤±è´¥: {str(e)}")
            return ''
    
    def test_nodes(self, nodes: List[str]) -> List[str]:
        """ä½¿ç”¨çº¿ç¨‹æ± åˆ†æ‰¹æµ‹è¯•èŠ‚ç‚¹"""
        self.logger.info(f"å¼€å§‹åˆ†æ‰¹æµ‹è¯•ï¼Œæ€»èŠ‚ç‚¹æ•°: {len(nodes)}")
        self.logger.info(f"æ‰¹æ¬¡å¤§å°: {self.batch_size}, å¹¶å‘æ‰¹æ¬¡æ•°: {self.max_workers}")
        
        # å°†èŠ‚ç‚¹åˆ†æˆæ‰¹æ¬¡
        batches = []
        for i in range(0, len(nodes), self.batch_size):
            batch = nodes[i:i + self.batch_size]
            batches.append(batch)
        
        self.logger.info(f"å…± {len(batches)} ä¸ªæ‰¹æ¬¡")
        
        # æ¸…ç†æ—§çš„æ‰¹æ¬¡é…ç½®æ–‡ä»¶
        self.clean_old_configs()
        
        # åˆ›å»ºç»Ÿä¸€é…ç½®æ–‡ä»¶ï¼ˆæ‰€æœ‰æ‰¹æ¬¡å…±äº«ï¼‰
        self.create_unified_config()
        
        # å¯åŠ¨HTTPæœåŠ¡å™¨
        http_server_port = 8888
        self.logger.info(f"å¯åŠ¨HTTPæœåŠ¡å™¨ï¼Œç«¯å£: {http_server_port}")
        http_server_process = subprocess.Popen(
            ['python3', '-m', 'http.server', str(http_server_port), '--directory', self.project_root],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL
        )
        
        time.sleep(5)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
        
        try:
            # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ‰¹æ¬¡
            all_results = []
            completed = 0
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {}
                
                # æäº¤æ‰€æœ‰æ‰¹æ¬¡
                for i, batch in enumerate(batches):
                    future = executor.submit(self.run_single_batch, batch, i, http_server_port)
                    futures[future] = i
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(futures):
                    batch_index = futures[future]
                    try:
                        results = future.result()
                        all_results.extend(results)
                        completed += 1
                        self.logger.info(f"æ‰¹æ¬¡ {batch_index} å®Œæˆï¼Œç´¯è®¡æœ‰æ•ˆèŠ‚ç‚¹: {len(all_results)}/{completed}/{len(batches)}")
                    except Exception as e:
                        self.logger.error(f"æ‰¹æ¬¡ {batch_index} å¤±è´¥: {str(e)}")
            
            self.logger.info(f"åˆ†æ‰¹æµ‹è¯•å®Œæˆï¼Œæ€»æœ‰æ•ˆèŠ‚ç‚¹: {len(all_results)}")
            return all_results
        
        finally:
            # åœæ­¢HTTPæœåŠ¡å™¨
            if http_server_process:
                http_server_process.terminate()
                http_server_process.wait(timeout=5)
                self.logger.info("HTTPæœåŠ¡å™¨å·²åœæ­¢")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='èŠ‚ç‚¹æµ‹é€Ÿè„šæœ¬ - ä½¿ç”¨çº¿ç¨‹æ± åˆ†æ‰¹æµ‹è¯•')
    parser.add_argument('--input', default='result/nodetotal.txt', help='è¾“å…¥èŠ‚ç‚¹æ–‡ä»¶')
    parser.add_argument('--output', default='result/nodelist.txt', help='è¾“å‡ºèŠ‚ç‚¹æ–‡ä»¶')
    parser.add_argument('--batch-size', type=int, default=100, help='æ¯æ‰¹èŠ‚ç‚¹æ•°')
    parser.add_argument('--max-workers', type=int, default=2, help='å¹¶å‘æ‰¹æ¬¡æ•°')
    
    args = parser.parse_args()
    
    logger = get_logger("main")
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.input):
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {args.input}")
        sys.exit(1)
    
    # è¯»å–èŠ‚ç‚¹
    logger.info(f"è¯»å–èŠ‚ç‚¹æ–‡ä»¶: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        nodes = [line.strip() for line in f if line.strip()]
    
    logger.info(f"è¯»å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
    
    # è¿è¡Œåˆ†æ‰¹æµ‹è¯•
    tester = BatchNodeTester()
    tester.batch_size = args.batch_size
    tester.max_workers = args.max_workers
    
    valid_nodes = tester.test_nodes(nodes)
    
    if valid_nodes:
        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(args.output), exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            for node in valid_nodes:
                f.write(f"{node}\n")
        logger.info(f"æœ‰æ•ˆèŠ‚ç‚¹å·²ä¿å­˜åˆ°: {args.output}")
    else:
        logger.warning("æœªæ‰¾åˆ°æœ‰æ•ˆèŠ‚ç‚¹")
    
    logger.info("æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()