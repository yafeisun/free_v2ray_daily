#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ”¶é›†å™¨ç®¡ç†å™¨
ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ”¶é›†å™¨çš„è¿è¡Œé€»è¾‘
"""

import sys
import os
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.config_manager import get_config
from src.collectors import get_collector_instance, run_collector
from src.utils.logger import get_logger
from src.utils.file_handler import FileHandler


class CollectorManager:
    """æ”¶é›†å™¨ç®¡ç†å™¨ - ç»Ÿä¸€ç®¡ç†æ‰€æœ‰æ”¶é›†å™¨çš„è¿è¡Œé€»è¾‘"""

    def __init__(self):
        self.config_manager = get_config()
        self.logger = get_logger("collector_manager")
        self.file_handler = FileHandler()
        self.collectors = {}
        self.results = {}

    def initialize_collectors(self, sites: Optional[List[str]] = None):
        """åˆå§‹åŒ–æ”¶é›†å™¨"""
        self.logger.info("åˆå§‹åŒ–æ”¶é›†å™¨...")

        websites = self.config_manager.websites.get_websites()

        for site_key, site_config in websites.items():
            if sites and site_key not in sites:
                self.logger.debug(f"è·³è¿‡æœªæŒ‡å®šçš„ç½‘ç«™: {site_key}")
                continue

            if not site_config.get("enabled", True):
                self.logger.debug(f"è·³è¿‡å·²ç¦ç”¨çš„ç½‘ç«™: {site_key}")
                continue

            collector = get_collector_instance(site_key, site_config)
            if collector:
                self.collectors[site_key] = collector
                self.logger.info(f"âœ“ åˆå§‹åŒ–æ”¶é›†å™¨: {site_config['name']}")
            else:
                self.logger.warning(f"âœ— åˆå§‹åŒ–å¤±è´¥: {site_config['name']}")

        self.logger.info(f"æˆåŠŸåˆå§‹åŒ– {len(self.collectors)} ä¸ªæ”¶é›†å™¨")
        return len(self.collectors) > 0

    def get_available_sites(self) -> List[str]:
        """è·å–æ‰€æœ‰å¯ç”¨ç½‘ç«™åˆ—è¡¨"""
        websites = self.config_manager.websites.get_websites()
        return [
            site_key
            for site_key, config in websites.items()
            if config.get("enabled", True)
        ]

    def get_plugin_info(self) -> Dict[str, Dict]:
        """è·å–æ’ä»¶ä¿¡æ¯"""
        from src.core.plugin_registry import get_registry

        registry = get_registry()

        websites = self.config_manager.websites.get_websites()
        info = {}

        for site_key, site_config in websites.items():
            if not site_config.get("enabled", True):
                continue

            metadata = registry.get_collector_metadata(site_key)
            info[site_key] = {
                "collector_class": metadata.get("class_name", "Unknown")
                if metadata
                else "Unknown",
                "module": metadata.get("module", "Unknown") if metadata else "Unknown",
                "description": metadata.get("description", "No description")
                if metadata
                else "No description",
                "enabled": site_config.get("enabled", True),
            }

        return info

    def collect_all_links(self) -> Dict[str, Dict]:
        """
        é˜¶æ®µ1ï¼šæ”¶é›†æ‰€æœ‰ç½‘ç«™çš„æ–‡ç« é“¾æ¥å’Œè®¢é˜…é“¾æ¥ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Returns:
            é“¾æ¥æ”¶é›†ç»“æœå­—å…¸
        """
        results = {}
        max_retries = 3
        retry_delay = 2  # ç§’

        for site_key, collector in self.collectors.items():
            success = False
            last_error = None

            # é‡è¯•æœºåˆ¶
            for attempt in range(max_retries):
                try:
                    # åªåœ¨é‡è¯•æ—¶æ˜¾ç¤ºå°è¯•ä¿¡æ¯
                    if attempt > 0:
                        self.logger.info(
                            f"ğŸ“„ é‡æ–°æ”¶é›† {collector.site_name} çš„é“¾æ¥... (å°è¯• {attempt + 1}/{max_retries})"
                        )
                    else:
                        self.logger.info(f"ğŸ“„ æ”¶é›† {collector.site_name} çš„é“¾æ¥...")

                    # åªæ”¶é›†é“¾æ¥ï¼Œä¸è§£æè®¢é˜…å†…å®¹
                    links_info = collector.collect_links()

                    if links_info and links_info.get("subscription_links"):
                        results[site_key] = {
                            "name": collector.site_name,
                            "article_url": links_info.get("article_url"),
                            "subscription_links": links_info.get(
                                "subscription_links", []
                            ),
                            "raw_data": links_info.get("raw_data"),
                            "success": True,
                        }
                        self.logger.info(
                            f"âœ“ {collector.site_name} æ‰¾åˆ° {len(links_info.get('subscription_links', []))} ä¸ªè®¢é˜…é“¾æ¥"
                        )
                        success = True
                        break
                    else:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è®¢é˜…é“¾æ¥ï¼Œä¹Ÿå¯èƒ½æ˜¯æ­£å¸¸æƒ…å†µï¼ˆç½‘ç«™æš‚æ—¶æ²¡æœ‰æ›´æ–°ï¼‰
                        results[site_key] = {
                            "name": collector.site_name,
                            "article_url": links_info.get("article_url")
                            if links_info
                            else None,
                            "subscription_links": [],
                            "raw_data": links_info.get("raw_data")
                            if links_info
                            else None,
                            "success": True,  # æˆåŠŸè®¿é—®ä½†æ²¡æœ‰æ–°å†…å®¹
                        }
                        self.logger.info(
                            f"âœ“ {collector.site_name} è®¿é—®æˆåŠŸä½†æœªæ‰¾åˆ°æ–°è®¢é˜…é“¾æ¥"
                        )
                        success = True
                        break

                except Exception as e:
                    last_error = str(e)
                    self.logger.warning(
                        f"âŒ {collector.site_name} é“¾æ¥æ”¶é›†å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {last_error}"
                    )

                    # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡å°è¯•ï¼Œç­‰å¾…åé‡è¯•
                    if attempt < max_retries - 1:
                        self.logger.info(f"â³ {retry_delay}ç§’åé‡è¯•...")
                        time.sleep(retry_delay)
                        retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    else:
                        # æœ€åä¸€æ¬¡å¤±è´¥ï¼Œè®°å½•é”™è¯¯
                        results[site_key] = {
                            "name": collector.site_name,
                            "success": False,
                            "error": last_error,
                        }
                        self.logger.error(
                            f"âŒ {collector.site_name} é“¾æ¥æ”¶é›†æœ€ç»ˆå¤±è´¥: {last_error}"
                        )

        return results

    def parse_all_subscriptions(
        self, links_results: Dict[str, Dict]
    ) -> Dict[str, Dict]:
        """
        é˜¶æ®µ2ï¼šç»Ÿä¸€è§£ææ‰€æœ‰è®¢é˜…é“¾æ¥

        Args:
            links_results: é˜¶æ®µ1çš„é“¾æ¥æ”¶é›†ç»“æœ

        Returns:
            æœ€ç»ˆçš„èŠ‚ç‚¹æ”¶é›†ç»“æœ
        """
        final_results = {}

        # æ”¶é›†æ‰€æœ‰è®¢é˜…é“¾æ¥è¿›è¡Œç»Ÿä¸€è§£æ
        all_subscription_links = []
        for site_key, site_data in links_results.items():
            if site_data.get("success") and site_data.get("subscription_links"):
                for link in site_data["subscription_links"]:
                    all_subscription_links.append(
                        {
                            "site_key": site_key,
                            "link": link,
                            "site_name": site_data["name"],
                        }
                    )

        self.logger.info(
            f"ğŸ” å…±æ”¶é›†åˆ° {len(all_subscription_links)} ä¸ªè®¢é˜…é“¾æ¥ï¼Œå¼€å§‹ç»Ÿä¸€è§£æ..."
        )

        # è§£ææ‰€æœ‰è®¢é˜…é“¾æ¥ï¼ˆå¸¦å®¹é”™æœºåˆ¶ï¼‰
        parsed_nodes = {}
        failed_links = 0
        total_parsed = 0

        for link_info in all_subscription_links:
            site_key = link_info["site_key"]
            link = link_info["link"]
            site_name = link_info["site_name"]

            try:
                self.logger.debug(f"è§£æ {site_name}: {link[:50]}...")
                nodes = self._parse_single_subscription_with_retry(link)

                if nodes:  # åªè®°å½•æœ‰å†…å®¹çš„è§£æç»“æœ
                    if site_key not in parsed_nodes:
                        parsed_nodes[site_key] = []
                    parsed_nodes[site_key].extend(nodes)
                    total_parsed += len(nodes)
                    self.logger.debug(f"âœ“ {site_name} è§£ææˆåŠŸ: {len(nodes)} ä¸ªèŠ‚ç‚¹")
                else:
                    self.logger.debug(f"âš ï¸ {site_name} è§£æä¸ºç©º: {link[:50]}...")

            except Exception as e:
                failed_links += 1
                self.logger.warning(
                    f"âŒ è®¢é˜…é“¾æ¥è§£æå¤±è´¥ {site_name}: {link[:50]}... - {str(e)}"
                )

        if failed_links > 0:
            success_rate = (
                (len(all_subscription_links) - failed_links)
                / len(all_subscription_links)
                * 100
            )
            self.logger.info(
                f"ğŸ“Š è§£æå®Œæˆ: {len(all_subscription_links) - failed_links}/{len(all_subscription_links)} æˆåŠŸ ({success_rate:.1f}%)"
            )

        # åˆå¹¶ç»“æœ
        for site_key, site_data in links_results.items():
            nodes = parsed_nodes.get(site_key, [])
            # é«˜çº§å»é‡ï¼ˆåŸºäºserver:portï¼‰
            unique_nodes = self._deduplicate_nodes_advanced(nodes)

            final_results[site_key] = {
                "name": site_data["name"],
                "nodes": unique_nodes,
                "article_url": site_data.get("article_url"),
                "subscription_links": site_data.get("subscription_links", []),
                "success": len(unique_nodes) > 0,
            }

            if unique_nodes:
                self.logger.info(
                    f"âœ“ {site_data['name']} è§£æå®Œæˆ: {len(unique_nodes)} ä¸ªèŠ‚ç‚¹ ({len(nodes)} â†’ {len(unique_nodes)} å»é‡)"
                )
            else:
                self.logger.warning(f"âš ï¸ {site_data['name']} æœªè§£æåˆ°èŠ‚ç‚¹")

        return final_results

    def _deduplicate_nodes_advanced(self, nodes: List[str]) -> List[str]:
        """é«˜çº§å»é‡ï¼šåŸºäºserver:portç»„åˆå»é‡"""
        return self._deduplicate_nodes(nodes)

    def _parse_single_subscription_with_retry(self, subscription_url: str) -> List[str]:
        """è§£æå•ä¸ªè®¢é˜…é“¾æ¥ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        max_retries = 2
        retry_delay = 1

        for attempt in range(max_retries):
            try:
                from src.core.subscription_parser import SubscriptionParser

                parser = SubscriptionParser()
                nodes = parser.parse_subscription_url(subscription_url)

                # éªŒè¯è§£æç»“æœ
                if nodes and isinstance(nodes, list):
                    return nodes
                else:
                    self.logger.debug(
                        f"è§£æç»“æœæ— æ•ˆ (å°è¯• {attempt + 1}): {type(nodes)}"
                    )
                    return []

            except Exception as e:
                if attempt < max_retries - 1:
                    self.logger.debug(f"è§£æé‡è¯• (å°è¯• {attempt + 1}): {str(e)}")
                    time.sleep(retry_delay)
                    retry_delay *= 2
                else:
                    raise e

        return []

    def _parse_single_subscription(self, subscription_url: str) -> List[str]:
        """è§£æå•ä¸ªè®¢é˜…é“¾æ¥ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰"""
        return self._parse_single_subscription_with_retry(subscription_url)

    def collect_all_sites(self, sites: Optional[List[str]] = None) -> Dict[str, Dict]:
        """
        æ”¶é›†æ‰€æœ‰ç½‘ç«™çš„èŠ‚ç‚¹ï¼ˆä¸¤é˜¶æ®µæµç¨‹ï¼‰

        Args:
            sites: æŒ‡å®šè¦æ”¶é›†çš„ç½‘ç«™åˆ—è¡¨ï¼Œä¸ºNoneæ—¶æ”¶é›†æ‰€æœ‰å¯ç”¨ç½‘ç«™

        Returns:
            æ”¶é›†ç»“æœå­—å…¸
        """
        self.logger.info("å¼€å§‹æ”¶é›†æ‰€æœ‰ç½‘ç«™...")

        # åˆå§‹åŒ–æ”¶é›†å™¨
        self.initialize_collectors(sites)

        if not self.collectors:
            self.logger.error("æ²¡æœ‰å¯ç”¨çš„æ”¶é›†å™¨")
            return {}

        # é˜¶æ®µ1ï¼šæ”¶é›†æ‰€æœ‰é“¾æ¥ï¼ˆæ–‡ç« URLå’Œè®¢é˜…é“¾æ¥ï¼‰
        self.logger.info("ğŸ“‹ é˜¶æ®µ1ï¼šæ”¶é›†æ–‡ç« é“¾æ¥å’Œè®¢é˜…é“¾æ¥...")
        links_results = self.collect_all_links()

        # é˜¶æ®µ2ï¼šç»Ÿä¸€è§£ææ‰€æœ‰è®¢é˜…é“¾æ¥
        self.logger.info("ğŸ” é˜¶æ®µ2ï¼šç»Ÿä¸€è§£æè®¢é˜…é“¾æ¥...")
        final_results = self.parse_all_subscriptions(links_results)

        total_nodes = sum(
            len(result.get("nodes", [])) for result in final_results.values()
        )
        self.logger.info(f"æ‰€æœ‰ç½‘ç«™æ”¶é›†å®Œæˆï¼Œå…±è·å– {total_nodes} ä¸ªèŠ‚ç‚¹")

        return final_results

    def run_single_collector(self, site_key: str) -> Tuple[bool, List[str]]:
        """è¿è¡Œå•ä¸ªæ”¶é›†å™¨"""
        if site_key not in self.collectors:
            self.logger.error(f"æ”¶é›†å™¨ä¸å­˜åœ¨: {site_key}")
            return False, []

        collector = self.collectors[site_key]
        site_name = collector.site_name

        try:
            self.logger.info(f"ğŸš€ å¼€å§‹æ”¶é›† {site_name}...")
            start_time = time.time()

            # è¿è¡Œæ”¶é›†å™¨
            nodes = collector.collect()

            # è®°å½•ç»“æœ
            duration = time.time() - start_time
            self.results[site_key] = {
                "success": bool(nodes),
                "node_count": len(nodes),
                "duration": duration,
                "nodes": nodes,
            }

            if nodes:
                self.logger.info(
                    f"âœ… {site_name} å®Œæˆï¼Œæ”¶é›†åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹ï¼Œè€—æ—¶ {duration:.2f}s"
                )
                return True, nodes
            else:
                self.logger.warning(f"âš ï¸ {site_name} æœªæ”¶é›†åˆ°èŠ‚ç‚¹ï¼Œè€—æ—¶ {duration:.2f}s")
                return False, []

        except Exception as e:
            self.logger.error(f"âŒ {site_name} è¿è¡Œå¼‚å¸¸: {str(e)}")
            self.results[site_key] = {
                "success": False,
                "node_count": 0,
                "duration": 0,
                "error": str(e),
                "nodes": [],
            }
            return False, []

    def run_all_collectors(self) -> Dict[str, List[str]]:
        """è¿è¡Œæ‰€æœ‰æ”¶é›†å™¨"""
        self.logger.info("ğŸš€ å¼€å§‹è¿è¡Œæ‰€æœ‰æ”¶é›†å™¨...")
        start_time = time.time()

        all_nodes = []
        success_count = 0
        total_count = len(self.collectors)

        for i, site_key in enumerate(self.collectors, 1):
            site_name = self.collectors[site_key].site_name
            self.logger.info(f"\n[{i}/{total_count}] {site_name}")
            self.logger.info("=" * 50)

            success, nodes = self.run_single_collector(site_key)
            if success:
                success_count += 1
                all_nodes.extend(nodes)

            # è¯·æ±‚é—´éš”
            if i < total_count:
                time.sleep(self.config_manager.base.REQUEST_DELAY)

        # å»é‡èŠ‚ç‚¹
        unique_nodes = self._deduplicate_nodes(all_nodes)

        # ç»Ÿè®¡ç»“æœ
        duration = time.time() - start_time
        duplicate_count = len(all_nodes) - len(unique_nodes)

        self.logger.info("\n" + "=" * 50)
        self.logger.info("ğŸ“Š æ”¶é›†ç»“æœç»Ÿè®¡:")
        self.logger.info(f"æ€»ç½‘ç«™æ•°: {total_count}")
        self.logger.info(f"æˆåŠŸç½‘ç«™æ•°: {success_count}")
        self.logger.info(f"å¤±è´¥ç½‘ç«™æ•°: {total_count - success_count}")
        self.logger.info(f"åŸå§‹èŠ‚ç‚¹æ•°: {len(all_nodes)}")
        self.logger.info(f"å»é‡èŠ‚ç‚¹æ•°: {len(unique_nodes)}")
        self.logger.info(f"é‡å¤èŠ‚ç‚¹æ•°: {duplicate_count}")
        self.logger.info(f"æ€»è€—æ—¶: {duration:.2f}s")
        self.logger.info("=" * 50)

        return {site_key: self.results[site_key]["nodes"] for site_key in self.results}

    def _deduplicate_nodes(self, nodes: List[str]) -> List[str]:
        """å»é‡èŠ‚ç‚¹ï¼ŒåŸºäºserver:portç»„åˆ"""
        if not nodes:
            return []

        seen = set()
        unique_nodes = []

        for node in nodes:
            server_port = self._extract_server_port(node)
            if server_port and server_port not in seen:
                seen.add(server_port)
                unique_nodes.append(node)

        return unique_nodes

    def _extract_server_port(self, node: str) -> Optional[str]:
        """ä»èŠ‚ç‚¹ä¸­æå–server:portä½œä¸ºå”¯ä¸€æ ‡è¯†"""
        try:
            if "://" not in node:
                return None

            protocol = node.split("://", 1)[0]
            rest = node.split("://", 1)[1]

            # ç§»é™¤åç§°éƒ¨åˆ†
            if "#" in rest:
                rest = rest.rsplit("#", 1)[0]

            # æå–server:port
            if "@" in rest:
                rest = rest.split("@", 1)[1]

            if ":" in rest:
                parts = rest.split(":")
                if len(parts) >= 2:
                    server = parts[0]
                    port = parts[1].split("?")[0].split("/")[0].rstrip("/")
                    return f"{server}:{port}"

        except Exception:
            pass

        return None

    def get_results_summary(self) -> Dict:
        """è·å–æ”¶é›†ç»“æœæ‘˜è¦"""
        if not self.results:
            return {}

        summary = {
            "total_sites": len(self.results),
            "successful_sites": sum(
                1 for r in self.results.values() if r.get("success", False)
            ),
            "total_nodes": sum(r.get("node_count", 0) for r in self.results.values()),
            "total_duration": sum(r.get("duration", 0) for r in self.results.values()),
            "sites": {},
        }

        for site_key, result in self.results.items():
            site_name = (
                self.collectors.get(site_key, {}).site_name
                if site_key in self.collectors
                else site_key
            )
            summary["sites"][site_name] = {
                "success": result.get("success", False),
                "node_count": result.get("node_count", 0),
                "duration": result.get("duration", 0),
                "error": result.get("error"),
            }

        return summary

    def list_available_collectors(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ”¶é›†å™¨"""
        collectors_info = []

        websites = self.config_manager.websites.get_websites()

        for site_key, site_config in websites.items():
            collectors_info.append(
                {
                    "key": site_key,
                    "name": site_config["name"],
                    "enabled": site_config.get("enabled", True),
                    "url": site_config["url"],
                }
            )

        return collectors_info

    def test_collectors(self) -> Dict[str, bool]:
        """æµ‹è¯•æ‰€æœ‰æ”¶é›†å™¨çš„å¯¼å…¥"""
        self.logger.info("ğŸ§ª æµ‹è¯•æ”¶é›†å™¨å¯¼å…¥...")

        test_results = {}
        websites = self.config_manager.websites.get_websites()

        for site_key, site_config in websites.items():
            try:
                collector = get_collector_instance(site_key, site_config)
                if collector:
                    self.logger.info(f"âœ… {site_config['name']} å¯¼å…¥æˆåŠŸ")
                    test_results[site_key] = True
                else:
                    self.logger.warning(f"âš ï¸ {site_config['name']} è·å–å¤±è´¥")
                    test_results[site_key] = False
            except Exception as e:
                self.logger.error(f"âŒ {site_config['name']} å¯¼å…¥å¤±è´¥: {e}")
                test_results[site_key] = False

        success_count = sum(test_results.values())
        websites_count = len(self.config_manager.websites.get_websites())
        self.logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{websites_count} æˆåŠŸ")

        return test_results
