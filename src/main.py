#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»ç¨‹åºå…¥å£
"""

import sys
import os
import time
import re
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# ä½¿ç”¨ç»å¯¹å¯¼å…¥
import sys
import os
import time
import re
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

# å¯¼å…¥å·¥å…·æ¨¡å—
try:
    from utils.logger import get_logger
    from utils.file_handler import FileHandler
    from config.settings import *
    from config.websites import WEBSITES
except ImportError as e:
    print(f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•è¿è¡Œ")
    sys.exit(1)

# å¯¼å…¥å„ä¸ªç½‘ç«™çš„çˆ¬è™«
from src.collectors.freeclashnode import FreeClashNodeCollector
from src.collectors.mibei77 import Mibei77Collector
from src.collectors.clashnodev2ray import ClashNodeV2RayCollector
from src.collectors.proxyqueen import ProxyQueenCollector
from src.collectors.wanzhuanmi import WanzhuanmiCollector
from src.collectors.cfmem import CfmemCollector
from src.collectors.clashnodecc import ClashNodeCCCollector
from src.collectors.datiya import DatiyaCollector
from src.collectors.telegeam import TelegeamCollector
from src.collectors.clashgithub import ClashGithubCollector
from src.collectors.freev2raynode import FreeV2rayNodeCollector
from src.collectors.eighty_five_la import EightyFiveLaCollector
from src.collectors.oneclash import OneClashCollector


class NodeCollector:
    """èŠ‚ç‚¹æ”¶é›†å™¨ä¸»ç±»"""

    def __init__(self):
        self.logger = get_logger("main")
        self.file_handler = FileHandler()

        # åˆå§‹åŒ–çˆ¬è™«
        self.collectors = {
            "freeclashnode": FreeClashNodeCollector(WEBSITES["freeclashnode"]),
            "mibei77": Mibei77Collector(WEBSITES["mibei77"]),
            "clashnodev2ray": ClashNodeV2RayCollector(WEBSITES["clashnodev2ray"]),
            "proxyqueen": ProxyQueenCollector(WEBSITES["proxyqueen"]),
            "wanzhuanmi": WanzhuanmiCollector(WEBSITES["wanzhuanmi"]),
            "cfmem": CfmemCollector(WEBSITES["cfmem"]),
            "clashnodecc": ClashNodeCCCollector(WEBSITES["clashnodecc"]),
            "datiya": DatiyaCollector(WEBSITES["datiya"]),
            "telegeam": TelegeamCollector(WEBSITES["telegeam"]),
            "clashgithub": ClashGithubCollector(WEBSITES["clashgithub"]),
            "freev2raynode": FreeV2rayNodeCollector(WEBSITES["freev2raynode"]),
            "85la": EightyFiveLaCollector(WEBSITES["85la"]),
            "oneclash": OneClashCollector(WEBSITES["oneclash"]),
        }

        self.all_nodes = []
        self.v2ray_subscription_links = []
        self.v2ray_links_with_source = []
        self.articles_with_source = []
        self.source_info = {}

    def collect_all_nodes(self):
        """æ”¶é›†æ‰€æœ‰ç½‘ç«™çš„æ–‡ç« å’Œè®¢é˜…é“¾æ¥ - ä¸¤é˜¶æ®µæ‰§è¡Œç­–ç•¥"""
        self.logger.info("=" * 50)
        self.logger.info("ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰ç½‘ç«™çš„æ–‡ç« é“¾æ¥å’Œè®¢é˜…é“¾æ¥")
        self.logger.info("=" * 50)

        start_time = time.time()
        today = datetime.now().strftime("%Y-%m-%d")
        date_str = datetime.now().strftime("%Y%m%d")

        # åˆ›å»ºæ—¥æœŸç›®å½•
        date_dir = f"result/{date_str}"
        os.makedirs(date_dir, exist_ok=True)

        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰ç½‘ç«™çš„æ–‡ç« é“¾æ¥å’Œè®¢é˜…é“¾æ¥
        for site_name, collector in self.collectors.items():
            try:
                self.logger.info(f"æ­£åœ¨æ”¶é›† {site_name} çš„æ–‡ç« å’Œè®¢é˜…é“¾æ¥...")

                # è·å–æœ€æ–°æ–‡ç« URL
                article_url = collector.get_latest_article_url()

                # è·å–V2Rayè®¢é˜…é“¾æ¥
                v2ray_links = []
                if article_url:
                    v2ray_links = collector.get_v2ray_subscription_links(article_url)

                # åˆå¹¶ä¿å­˜æ–‡ç« é“¾æ¥å’Œè®¢é˜…é“¾æ¥åˆ°ä¸€ä¸ªæ–‡ä»¶
                info_file = os.path.join(date_dir, f"{site_name}_info.txt")
                with open(info_file, "w", encoding="utf-8") as f:
                    f.write(f"# {site_name} æ–‡ç« å’Œè®¢é˜…é“¾æ¥\n")
                    f.write(
                        f"# æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                    )
                    f.write("-" * 60 + "\n\n")

                    # æ–‡ç« é“¾æ¥éƒ¨åˆ†
                    f.write("## æ–‡ç« é“¾æ¥\n")
                    if article_url:
                        f.write(f"{article_url}\n")
                        self.articles_with_source.append(
                            {
                                "website_name": site_name,
                                "article_url": article_url,
                                "date": today,
                            }
                        )
                    else:
                        f.write("# ä»Šæ—¥æœªæ›´æ–°æ–‡ç« \n")
                    f.write("\n")

                    # è®¢é˜…é“¾æ¥éƒ¨åˆ†
                    f.write("## è®¢é˜…é“¾æ¥\n")
                    if v2ray_links:
                        for link in v2ray_links:
                            f.write(f"{link}\n")
                    else:
                        f.write("# æ— V2Rayè®¢é˜…é“¾æ¥\n")

                self.logger.info(
                    f"{site_name}: æ–‡ç« å’Œè®¢é˜…é“¾æ¥å·²ä¿å­˜åˆ° {info_file}ï¼Œå…± {len(v2ray_links)} ä¸ªé“¾æ¥"
                )

                # è®°å½•V2Rayè®¢é˜…é“¾æ¥ä¿¡æ¯ï¼ˆç”¨äºæ±‡æ€»ï¼‰
                if not v2ray_links:
                    self.v2ray_links_with_source.append(
                        {
                            "url": "# æ— V2Rayè®¢é˜…é“¾æ¥",
                            "source": site_name,
                            "source_url": article_url if article_url else "N/A",
                        }
                    )
                else:
                    for link in v2ray_links:
                        self.v2ray_links_with_source.append(
                            {
                                "url": link,
                                "source": site_name,
                                "source_url": article_url,
                            }
                        )

                # æ”¶é›†æ‰€æœ‰è®¢é˜…é“¾æ¥ï¼ˆä¸å»é‡ï¼Œä¿ç•™ç½‘ç«™ä¿¡æ¯ï¼‰
                self.v2ray_subscription_links.extend(v2ray_links)

                # æ›´æ–°æºä¿¡æ¯
                self.source_info[site_name] = {
                    "count": 0,  # ç¨åæ›´æ–°
                    "enabled": collector.enabled,
                    "subscription_links": len(v2ray_links),
                    "v2ray_links": len(v2ray_links),
                    "links": v2ray_links[:5],
                    "v2ray_link_samples": v2ray_links[:5],
                }

                # è¯·æ±‚é—´éš”
                if site_name != list(self.collectors.keys())[-1]:
                    time.sleep(REQUEST_DELAY)

            except Exception as e:
                self.logger.error(f"æ”¶é›† {site_name} æ—¶å‡ºé”™: {str(e)}")
                # å³ä½¿å‡ºé”™ä¹Ÿä¿å­˜é”™è¯¯ä¿¡æ¯
                article_file = os.path.join(date_dir, f"{site_name}_article.txt")
                with open(article_file, "w", encoding="utf-8") as f:
                    f.write(f"# {site_name} æ–‡ç« é“¾æ¥\n")
                    f.write(
                        f"# æ›´æ–°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                    )
                    f.write(f"# çŠ¶æ€: æ”¶é›†å¤±è´¥\n")
                    f.write(f"# é”™è¯¯ä¿¡æ¯: {str(e)}\n")
                    f.write("-" * 60 + "\n\n")

        # ç¬¬ä¸€é˜¶æ®µå®Œæˆ
        first_phase_time = time.time() - start_time
        self.logger.info(f"ç¬¬ä¸€é˜¶æ®µå®Œæˆï¼Œè€—æ—¶: {first_phase_time:.2f}ç§’")
        self.logger.info(f"æ”¶é›†åˆ° {len(self.v2ray_subscription_links)} ä¸ªè®¢é˜…é“¾æ¥")

        # ç¬¬äºŒé˜¶æ®µï¼šé€šè¿‡è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹
        self.logger.info("=" * 50)
        self.logger.info("ç¬¬äºŒé˜¶æ®µï¼šé€šè¿‡è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹")
        self.logger.info("=" * 50)

        second_start_time = time.time()

        # å»é‡è®¢é˜…é“¾æ¥ï¼ˆä¿ç•™ç½‘ç«™ä¿¡æ¯ï¼‰
        unique_links = []
        seen_links = set()
        links_with_source = []

        for link_info in self.v2ray_links_with_source:
            if (
                link_info["url"] not in seen_links
                and link_info["url"] != "# æ— V2Rayè®¢é˜…é“¾æ¥"
            ):
                seen_links.add(link_info["url"])
                unique_links.append(link_info["url"])
                links_with_source.append(link_info)

        self.logger.info(f"å»é‡åæœ‰ {len(unique_links)} ä¸ªå”¯ä¸€è®¢é˜…é“¾æ¥")

        # é€šè¿‡è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹
        for i, link_info in enumerate(links_with_source):
            try:
                self.logger.info(
                    f"å¤„ç†è®¢é˜…é“¾æ¥ ({i + 1}/{len(links_with_source)}): {link_info['url'][:50]}..."
                )

                # è·å–æ”¶é›†å™¨å®ä¾‹
                collector = self.collectors[link_info["source"]]

                # ä»è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹
                nodes = collector.get_nodes_from_subscription(link_info["url"])

                if nodes:
                    self.all_nodes.extend(nodes)
                    self.logger.info(
                        f"ä» {link_info['source']} è·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹"
                    )

                    # æ›´æ–°æºä¿¡æ¯ä¸­çš„èŠ‚ç‚¹æ•°
                    if link_info["source"] in self.source_info:
                        self.source_info[link_info["source"]]["count"] += len(nodes)
                else:
                    self.logger.warning(f"ä» {link_info['url']} æœªè·å–åˆ°èŠ‚ç‚¹")

                # è¯·æ±‚é—´éš”
                if i < len(links_with_source) - 1:
                    time.sleep(REQUEST_DELAY)

            except Exception as e:
                self.logger.error(f"å¤„ç†è®¢é˜…é“¾æ¥å¤±è´¥ {link_info['url']}: {str(e)}")

        # å»é‡ï¼ˆä½¿ç”¨server+portç»„åˆä½œä¸ºå»é‡æ ‡å‡†ï¼Œå»é™¤çœŸæ­£é‡å¤çš„èŠ‚ç‚¹ï¼‰
        original_count = len(self.all_nodes)
        seen = set()
        unique_nodes = []
        for node in self.all_nodes:
            # æå–server+portä½œä¸ºå”¯ä¸€æ ‡è¯†
            server_port = None
            try:
                if "://" in node:
                    protocol = node.split("://", 1)[0]
                    rest = node.split("://", 1)[1]

                    # ç§»é™¤åç§°éƒ¨åˆ†ï¼ˆ#åé¢çš„å†…å®¹ï¼‰
                    if "#" in rest:
                        rest = rest.rsplit("#", 1)[0]

                    # æå–server+port
                    if "@" in rest:
                        rest = rest.split("@", 1)[1]

                    if ":" in rest:
                        parts = rest.split(":")
                        if len(parts) >= 2:
                            server = parts[0]
                            port = (
                                parts[1]
                                .split("?")[0]
                                .split("/")[0]
                                .split("\\")[0]
                                .rstrip("/")
                            )
                            server_port = f"{server}:{port}"
            except:
                pass

            # å¦‚æœæ— æ³•æå–server+portï¼Œä½¿ç”¨å®Œæ•´èŠ‚ç‚¹ä½œä¸ºæ ‡è¯†
            if not server_port:
                server_port = node

            if server_port not in seen:
                seen.add(server_port)
                unique_nodes.append(node)

        self.all_nodes = unique_nodes
        duplicate_count = original_count - len(self.all_nodes)

        # ä¸ºæ”¶é›†é˜¶æ®µçš„èŠ‚ç‚¹ç”Ÿæˆç®€å•å‘½åï¼ˆå›½æ——_åŒºåŸŸ_æ•°å­—ï¼‰
        named_nodes = []
        region_counters = {}

        for i, node in enumerate(self.all_nodes):
            # æå–åœ°åŒºä¿¡æ¯ç”¨äºç”Ÿæˆç®€å•åç§°
            region = self._extract_region_for_collection(node)

            # åˆå§‹åŒ–åœ°åŒºè®¡æ•°å™¨
            if region not in region_counters:
                region_counters[region] = 0

            # åœ°åŒºç¼–å·é€’å¢
            region_counters[region] += 1
            region_number = region_counters[region]

            # ç”Ÿæˆç®€å•åç§°
            simple_name = self._generate_simple_node_name(region, region_number)

            # æ·»åŠ åç§°åˆ°èŠ‚ç‚¹ï¼ˆå¦‚æœèŠ‚ç‚¹æœ‰åç§°éƒ¨åˆ†ï¼‰
            if "#" in node:
                node_with_name = node.rsplit("#", 1)[0] + f"#{simple_name}"
            else:
                node_with_name = f"{node}#{simple_name}"

            named_nodes.append(node_with_name)

        # ä¿å­˜å»é‡å¹¶é‡å‘½ååçš„æ‰€æœ‰èŠ‚ç‚¹åˆ° nodetotal.txt
        nodetotal_file = os.path.join(date_dir, "nodetotal.txt")
        with open(nodetotal_file, "w", encoding="utf-8") as f:
            for node in named_nodes:
                f.write(f"{node}\n")

        self.logger.info(
            f"æ‰€æœ‰èŠ‚ç‚¹å·²ä¿å­˜åˆ° {nodetotal_file}ï¼ŒåŸå§‹: {original_count} ä¸ªï¼Œå»é‡å: {len(self.all_nodes)} ä¸ªï¼Œé‡å¤: {duplicate_count} ä¸ª"
        )

        end_time = time.time()
        total_duration = end_time - start_time
        second_phase_time = end_time - second_start_time

        self.logger.info("=" * 50)
        self.logger.info("èŠ‚ç‚¹æ”¶é›†å®Œæˆ")
        self.logger.info(f"æ€»æ”¶é›†æ—¶é—´: {total_duration:.2f}ç§’")
        self.logger.info(f"ç¬¬ä¸€é˜¶æ®µæ—¶é—´: {first_phase_time:.2f}ç§’")
        self.logger.info(f"ç¬¬äºŒé˜¶æ®µæ—¶é—´: {second_phase_time:.2f}ç§’")
        self.logger.info(f"åŸå§‹èŠ‚ç‚¹æ•°: {original_count}")
        self.logger.info(f"å»é‡åèŠ‚ç‚¹æ•°: {len(self.all_nodes)}")
        self.logger.info(f"é‡å¤èŠ‚ç‚¹æ•°: {duplicate_count}")
        self.logger.info(f"å”¯ä¸€è®¢é˜…é“¾æ¥æ•°: {len(unique_links)}")
        self.logger.info("=" * 50)

        return self.all_nodes

    def update_github(self):
        """æ›´æ–°GitHubä»“åº“"""
        try:
            import git

            repo_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            os.chdir(repo_path)

            repo = git.Repo(repo_path)

            with repo.config_writer() as cw:
                cw.set_value("user", "email", GIT_EMAIL)
                cw.set_value("user", "name", GIT_NAME)

            if repo.is_dirty(untracked_files=True):
                repo.index.add(["result/nodelist.txt", "result/nodetotal.txt"])

                # ç”Ÿæˆè¯¦ç»†çš„æäº¤ä¿¡æ¯
                date_str = datetime.now().strftime("%Y-%m-%d")
                date_dir = f"result/{date_str}"
                update_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                commit_lines = [f"æ›´æ–°èŠ‚ç‚¹åˆ—è¡¨ - {update_time}"]
                commit_lines.append("=" * 60)
                commit_lines.append(f"æ›´æ–°æ—¶é—´: {update_time}")

                # æ±‡æ€»ç½‘ç«™æ”¶é›†æƒ…å†µ
                commit_lines.append("\nç½‘ç«™æ”¶é›†æƒ…å†µ:")
                success_sites = []
                failed_sites = []

                for site_name in self.collectors.keys():
                    info_file = os.path.join(date_dir, f"{site_name}_info.txt")
                    if os.path.exists(info_file):
                        success_sites.append(site_name)
                        # è¯»å– info æ–‡ä»¶å†…å®¹
                        with open(info_file, "r", encoding="utf-8") as f:
                            content = f.read()
                            # æå–æ–‡ç« é“¾æ¥
                            article_url = None
                            for line in content.split("\n"):
                                if line.startswith("## æ–‡ç« é“¾æ¥\n"):
                                    next_line = (
                                        content.split("## æ–‡ç« é“¾æ¥\n")[1].split("\n")[1]
                                        if "## æ–‡ç« é“¾æ¥\n" in content
                                        else ""
                                    )
                                    if next_line and not next_line.startswith("#"):
                                        article_url = next_line.strip()
                                    break

                            # æå–è®¢é˜…é“¾æ¥æ•°é‡
                            subscription_count = 0
                            for line in content.split("\n"):
                                if line.startswith("## è®¢é˜…é“¾æ¥\n"):
                                    subscription_section = (
                                        content.split("## è®¢é˜…é“¾æ¥\n")[1].split("\n")[
                                            1:
                                        ]
                                        if "## è®¢é˜…é“¾æ¥\n" in content
                                        else []
                                    )
                                    subscription_count = len(
                                        [
                                            l
                                            for l in subscription_section
                                            if l.strip() and not l.startswith("#")
                                        ]
                                    )
                                    break

                            commit_lines.append(f"\nâœ“ {site_name}:")
                            if article_url:
                                commit_lines.append(f"  æ–‡ç« : {article_url}")
                            if subscription_count > 0:
                                commit_lines.append(
                                    f"  è®¢é˜…é“¾æ¥: {subscription_count} ä¸ª"
                                )
                            else:
                                commit_lines.append(f"  è®¢é˜…é“¾æ¥: æ— ")
                    else:
                        failed_sites.append(site_name)
                        commit_lines.append(f"\nâœ— {site_name}: æœªè·å–åˆ°æ•°æ®")

                # æ±‡æ€»ç»Ÿè®¡
                commit_lines.append("\n" + "=" * 60)
                commit_lines.append(f"æˆåŠŸ: {len(success_sites)} ä¸ªç½‘ç«™")
                commit_lines.append(f"å¤±è´¥: {len(failed_sites)} ä¸ªç½‘ç«™")
                if failed_sites:
                    commit_lines.append(f"å¤±è´¥ç½‘ç«™: {', '.join(failed_sites)}")

                # èŠ‚ç‚¹ç»Ÿè®¡
                node_count = len(self.all_nodes)
                commit_lines.append(f"\næ€»èŠ‚ç‚¹æ•°: {node_count}")

                # ç»„åˆæäº¤ä¿¡æ¯
                commit_message = "\n".join(commit_lines)

                repo.index.commit(commit_message)

                origin = repo.remote(name="origin")
                origin.push()

                self.logger.info(f"æˆåŠŸæ¨é€åˆ°GitHub")
                self.logger.info(f"æäº¤ä¿¡æ¯:\n{commit_message}")
            else:
                self.logger.info("æ²¡æœ‰å˜åŒ–éœ€è¦æäº¤")

            return True

        except Exception as e:
            self.logger.error(f"æ›´æ–°GitHubå¤±è´¥: {str(e)}")
            return False

    def run(self, target_dates=None):
        """è¿è¡Œå®Œæ•´çš„æ”¶é›†æµç¨‹"""
        if target_dates is None:
            target_dates = [datetime.now()]

        try:
            # 1. æ”¶é›†æ‰€æœ‰ç½‘ç«™çš„èŠ‚ç‚¹ï¼ˆæ–°ç­–ç•¥ï¼šä¾æ¬¡æ‰§è¡Œï¼ŒæŒ‰ç½‘ç«™ä¿å­˜æ–‡ä»¶ï¼‰
            all_nodes = self.collect_all_nodes()

            if not all_nodes:
                self.logger.warning("æœªæ”¶é›†åˆ°ä»»ä½•èŠ‚ç‚¹")
                return False

            # 2. åŒæ­¥æ‰€æœ‰èŠ‚ç‚¹åˆ°æ ¹ç›®å½•
            date_suffix = datetime.now().strftime("%Y%m%d")
            sync_success = self.file_handler.sync_latest_to_root(date_suffix)

            if sync_success:
                self.logger.info(
                    f"æ‰€æœ‰èŠ‚ç‚¹å·²åŒæ­¥åˆ°æ ¹ç›®å½•: result/nodetotal.txt ({len(all_nodes)} ä¸ª)"
                )
            else:
                self.logger.warning("åŒæ­¥èŠ‚ç‚¹åˆ°æ ¹ç›®å½•å¤±è´¥")

            # 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            clean_success = self.file_handler.clean_root_temp_files()

            if clean_success:
                self.logger.info(f"å·²æ¸…ç†ä¸´æ—¶æ–‡ä»¶")

            self.logger.info("æ”¶é›†æµç¨‹å®Œæˆ")
            return True

        except Exception as e:
            self.logger.error(f"è¿è¡Œæ”¶é›†æµç¨‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

    def _load_existing_articles(self, date_str=None):
        """åŠ è½½ç°æœ‰çš„æ–‡ç« é“¾æ¥ç¼“å­˜"""
        # ä½¿ç”¨file_handlerçš„æ–°æ–¹æ³•
        date_suffix = None
        if date_str:
            date_suffix = date_str.replace("-", "")

        return self.file_handler.load_existing_articles(date_suffix)

    def _load_existing_subscriptions(self, date_str=None):
        """åŠ è½½ç°æœ‰çš„è®¢é˜…é“¾æ¥ç¼“å­˜"""
        # ä½¿ç”¨file_handlerçš„æ–°æ–¹æ³•
        date_suffix = None
        if date_str:
            date_suffix = date_str.replace("-", "")

        return self.file_handler.load_existing_subscriptions(date_suffix)

    def _find_existing_article(self, existing_articles, website_name, date_str):
        """æŸ¥æ‰¾ç°æœ‰çš„æ–‡ç« é“¾æ¥"""
        # ç›´æ¥ä½¿ç”¨ç½‘ç«™åç§°ä½œä¸ºé”®åï¼ˆä¸ç¼“å­˜è§£æé€»è¾‘ä¸€è‡´ï¼‰
        if website_name in existing_articles:
            return existing_articles[website_name]

        return None

    def _find_existing_subscriptions(
        self, existing_subscriptions, website_name, article_url
    ):
        """æŸ¥æ‰¾ç°æœ‰çš„è®¢é˜…é“¾æ¥"""
        key = f"{website_name}_{article_url}"
        return existing_subscriptions.get(key, [])

    def _extract_region_for_collection(self, node: str) -> str:
        """ä»èŠ‚ç‚¹ä¸­æå–åœ°åŒºä¿¡æ¯ï¼ˆæ”¶é›†é˜¶æ®µä½¿ç”¨ï¼‰"""
        try:
            # ç®€å•çš„åŸºäºç«¯å£çš„åœ°åŒºåˆ¤æ–­
            if ":" in node:
                # ç§»é™¤åè®®éƒ¨åˆ†
                rest = node.split("://", 1)[1] if "://" in node else node

                # ç§»é™¤åç§°éƒ¨åˆ†
                rest = rest.rsplit("#", 1)[0] if "#" in rest else rest

                # æå–ç«¯å£
                if "@" in rest:
                    rest = rest.split("@", 1)[1]

                if ":" in rest:
                    port_str = (
                        rest.split(":")[1].split("?")[0].split("/")[0].rstrip("/")
                    )
                    try:
                        port = int(port_str)

                        # åŸºäºç«¯å£èŒƒå›´åˆ¤æ–­åœ°åŒº
                        if 10000 <= port <= 19999:
                            return "US"  # ç¾å›½å¸¸ç”¨ç«¯å£èŒƒå›´
                        elif 20000 <= port <= 29999:
                            return "HK"  # é¦™æ¸¯å¸¸ç”¨ç«¯å£èŒƒå›´
                        elif 30000 <= port <= 39999:
                            return "JP"  # æ—¥æœ¬å¸¸ç”¨ç«¯å£èŒƒå›´
                        elif 40000 <= port <= 49999:
                            return "SG"  # æ–°åŠ å¡å¸¸ç”¨ç«¯å£èŒƒå›´
                        elif port >= 50000:
                            return "EU"  # æ¬§æ´²å¸¸ç”¨ç«¯å£èŒƒå›´
                    except ValueError:
                        pass

            # é»˜è®¤è¿”å›US
            return "US"
        except:
            return "US"

    def _generate_simple_node_name(self, region: str, number: int) -> str:
        """ç”Ÿæˆç®€å•èŠ‚ç‚¹åç§°ï¼ˆæ”¶é›†é˜¶æ®µä½¿ç”¨ï¼‰"""
        # å›½æ——æ˜ å°„
        flags = {
            "HK": "ğŸ‡­ğŸ‡°",
            "US": "ğŸ‡ºğŸ‡¸",
            "JP": "ğŸ‡¯ğŸ‡µ",
            "SG": "ğŸ‡¸ğŸ‡¬",
            "TW": "ğŸ‡¨ğŸ‡³",
            "KR": "ğŸ‡°ğŸ‡·",
            "DE": "ğŸ‡©ğŸ‡ª",
            "GB": "ğŸ‡¬ğŸ‡§",
            "FR": "ğŸ‡«ğŸ‡·",
            "CA": "ğŸ‡¨ğŸ‡¦",
            "NL": "ğŸ‡³ğŸ‡±",
            "RU": "ğŸ‡·ğŸ‡º",
            "IN": "ğŸ‡®ğŸ‡³",
            "BR": "ğŸ‡§ğŸ‡·",
            "AU": "ğŸ‡¦ğŸ‡º",
            "EU": "ğŸ‡ªğŸ‡º",
        }

        flag = flags.get(region, "ğŸ‡ºğŸ‡¸")

        # ç®€å•æ ¼å¼ï¼šå›½æ——_åŒºåŸŸ_æ•°å­—
        return f"{flag}{region}_{number}"

    def _get_nodes_from_subscription_links(self, subscription_links, collector):
        """ä»è®¢é˜…é“¾æ¥è·å–å®é™…èŠ‚ç‚¹"""
        nodes = []

        for link in subscription_links:
            try:
                self.logger.info(f"ä»è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹: {link}")
                sub_nodes = collector.get_nodes_from_subscription(link)
                nodes.extend(sub_nodes)
                self.logger.info(f"ä» {link} è·å–åˆ° {len(sub_nodes)} ä¸ªèŠ‚ç‚¹")
            except Exception as e:
                self.logger.error(f"ä»è®¢é˜…é“¾æ¥è·å–èŠ‚ç‚¹å¤±è´¥ {link}: {str(e)}")

        # å»é‡
        nodes = list(set(nodes))
        self.logger.info(f"ä»æ‰€æœ‰è®¢é˜…é“¾æ¥è·å–åˆ° {len(nodes)} ä¸ªå»é‡åçš„èŠ‚ç‚¹")

        return nodes

    def _extract_host_port_from_node(self, node):
        """ä»èŠ‚ç‚¹ä¿¡æ¯ä¸­æå–ä¸»æœºå’Œç«¯å£"""
        try:
            if node.startswith("vmess://"):
                import base64
                import json

                data = node[8:]
                data += "=" * (-len(data) % 4)
                decoded = base64.b64decode(data).decode("utf-8")
                config = json.loads(decoded)
                return config.get("add"), config.get("port")

            elif (
                node.startswith("vless://")
                or node.startswith("trojan://")
                or node.startswith("hysteria://")
            ):
                import urllib.parse

                parsed = urllib.parse.urlparse(node)
                return parsed.hostname, parsed.port

            elif node.startswith("ss://"):
                import urllib.parse
                import base64

                if "#" in node:
                    node = node.split("#")[0]

                parsed = urllib.parse.urlparse(node)

                if parsed.hostname and parsed.port:
                    return parsed.hostname, parsed.port
                else:
                    data = node[5:]
                    data += "=" * (-len(data) % 4)
                    decoded = base64.b64decode(data).decode("utf-8")

                    if ":" in decoded:
                        parts = decoded.split(":")
                        if len(parts) >= 2:
                            host = parts[0]
                            port_str = (
                                parts[1].split("@")[0] if "@" in parts[1] else parts[1]
                            )
                            return host, int(port_str)

            return None, None

        except Exception as e:
            self.logger.error(f"æå–ä¸»æœºç«¯å£å¤±è´¥: {str(e)}")
            return None, None


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="å…è´¹V2RayèŠ‚ç‚¹æ”¶é›†å™¨")
    parser.add_argument("--update-github", action="store_true", help="æ›´æ–°GitHubä»“åº“")
    parser.add_argument(
        "--sites", nargs="+", help="æŒ‡å®šè¦æ”¶é›†çš„ç½‘ç«™", choices=list(WEBSITES.keys())
    )
    parser.add_argument("--date", help="æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD (é»˜è®¤: ä»Šå¤©)")
    parser.add_argument("--dates", nargs="+", help="æŒ‡å®šå¤šä¸ªæ—¥æœŸï¼Œæ ¼å¼: YYYY-MM-DD")
    parser.add_argument("--days", type=int, help="è·å–æœ€è¿‘Nå¤©çš„æ•°æ®")

    args = parser.parse_args()

    # å¤„ç†æ—¥æœŸå‚æ•°
    target_dates = []

    if args.dates:
        for date_str in args.dates:
            try:
                target_dates.append(datetime.strptime(date_str, "%Y-%m-%d"))
            except ValueError:
                print(f"âŒ æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {date_str}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
                sys.exit(1)
    elif args.days:
        today = datetime.now()
        for i in range(args.days):
            target_dates.append(today - timedelta(days=i))
    elif args.date:
        try:
            target_dates.append(datetime.strptime(args.date, "%Y-%m-%d"))
        except ValueError:
            print(f"âŒ æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {args.date}ï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
            sys.exit(1)
    else:
        target_dates = None

    # åˆ›å»ºæ”¶é›†å™¨
    collector = NodeCollector()

    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šç½‘ç«™ï¼Œåªå¯ç”¨è¿™äº›ç½‘ç«™
    if args.sites:
        for site_name in collector.collectors:
            if site_name not in args.sites:
                collector.collectors[site_name].enabled = False
                collector.logger.info(f"ç¦ç”¨ç½‘ç«™: {site_name}")
    else:
        collector.logger.info("è¿è¡Œæ‰€æœ‰ç½‘ç«™")

    # è¿è¡Œæ”¶é›†
    success = collector.run(target_dates=target_dates)

    # æ›´æ–°GitHub
    if success and args.update_github:
        collector.update_github()

    if success:
        print("âœ“ ä»»åŠ¡å®Œæˆ")
        sys.exit(0)
    else:
        print("âœ— ä»»åŠ¡å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
